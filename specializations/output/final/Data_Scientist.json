{
  "profile": "Data Scientist",
  "specialization": "Data Science",
  "file_name": "Data_Scientist",
  "competencies": [
    {
      "competency": "Статистика и теория вероятностей",
      "type": "CORE",
      "importance": 90,
      "themes": [
        {
          "theme": "Проверка статистических гипотез и A/B тестирование",
          "questions": [
            {
              "level": "Junior",
              "question": "Что такое p-value в контексте A/B теста и какой порог обычно используется для определения статистической значимости?",
              "correct_answer": "Вероятность получить результат при истинной нулевой гипотезе, порог 0.05",
              "var_1": "Мощность теста для обнаружения различий, обычно используется 0.8",
              "var_2": "Доля пользователей с изменением конверсии, критический уровень 0.1",
              "var_3": "Вероятность того, что гипотеза верна, стандартный порог 0.01",
              "var_4": "Вероятность получить результат при истинной нулевой гипотезе, порог 0.05",
              "correct_position": 4
            },
            {
              "level": "Middle",
              "question": "В банке тестируют новый интерфейс мобильного приложения. Какой статистический тест использовать для сравнения конверсии в оформление кредита между группами, если размер выборки 5000 пользователей в каждой группе?",
              "correct_answer": "Z-тест для разности двух пропорций или хи-квадрат тест",
              "var_1": "Тест Манна-Уитни для непараметрического сравнения медиан",
              "var_2": "Парный t-тест для связанных выборок пользователей",
              "var_3": "ANOVA с post-hoc анализом для множественных групп",
              "var_4": "Z-тест для разности двух пропорций или хи-квадрат тест",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Телеком запускает A/B тест новой тарифной модели на 2% пользователей. Как спроектировать систему мониторинга для раннего обнаружения негативного эффекта, учитывая проблему множественных сравнений при ежедневной проверке метрик?",
              "correct_answer": "Sequential testing с adjusted alpha boundaries или Bayesian monitoring с предустановленными stopping rules",
              "var_1": "Sequential testing с adjusted alpha boundaries или Bayesian monitoring с предустановленными stopping rules",
              "var_2": "Многоуровневый мониторинг с False Discovery Rate control через процедуру Benjamini-Hochberg для p-values",
              "var_3": "Ежедневный t-test с Bonferroni correction для всех ключевых метрик и dashboard с алертами",
              "var_4": "CUSUM charts с фиксированными control limits и автоматическая остановка теста при первом отклонении",
              "correct_position": 1
            }
          ]
        },
        {
          "theme": "Распределения вероятностей и их применение в машинном обучении",
          "questions": [
            {
              "level": "Junior",
              "question": "Какое распределение вероятностей используется для моделирования бинарных исходов, например, дефолт/не дефолт клиента банка?",
              "correct_answer": "Распределение Бернулли",
              "var_1": "Биномиальное распределение",
              "var_2": "Распределение Пуассона",
              "var_3": "Распределение Бернулли",
              "var_4": "Нормальное распределение",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "Почему для моделирования времени между звонками в колл-центре телеком-оператора используют экспоненциальное распределение вместо нормального?",
              "correct_answer": "Экспоненциальное распределение моделирует время между независимыми событиями и определено только для положительных значений",
              "var_1": "Экспоненциальное распределение лучше отражает симметричность интервалов между входящими звонками клиентов",
              "var_2": "Центральная предельная теорема обеспечивает сходимость временных интервалов к экспоненциальному виду",
              "var_3": "Экспоненциальное распределение моделирует время между независимыми событиями и определено только для положительных значений",
              "var_4": "Нормальное распределение плохо работает с малыми выборками звонков в непиковые часы работы",
              "correct_position": 3
            },
            {
              "level": "Senior",
              "question": "Как выбор prior распределения в байесовской модели кредитного скоринга влияет на устойчивость предсказаний при дрейфе данных и какое семейство распределений предпочтительнее для small data сценариев?",
              "correct_answer": "Информативный prior со слабой регуляризацией стабилизирует оценки при дрейфе; conjugate prior семейства обеспечивают аналитические решения для малых выборок",
              "var_1": "Информативный prior со слабой регуляризацией стабилизирует оценки при дрейфе; conjugate prior семейства обеспечивают аналитические решения для малых выборок",
              "var_2": "Hierarchical prior с сильной регуляризацией компенсирует дрейф; empirical Bayes подход эффективен при малом количестве наблюдений",
              "var_3": "Uniform prior минимизирует bias при дрейфе; экспоненциальное семейство распределений оптимально для ограниченных данных в продакшене",
              "var_4": "Non-informative prior обеспечивает робастность к дрейфу; mixture distributions из Gaussian семейства предпочтительны для sparse data",
              "correct_position": 1
            }
          ]
        },
        {
          "theme": "Корреляционный и регрессионный анализ данных",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой коэффициент корреляции указывает на сильную положительную линейную связь между суммой кредита и доходом клиента банка?",
              "correct_answer": "Коэффициент Пирсона близкий к +1",
              "var_1": "Коэффициент детерминации R² близкий к единице",
              "var_2": "Коэффициент Спирмена для нелинейных зависимостей",
              "var_3": "Коэффициент Пирсона близкий к +1",
              "var_4": "Коэффициент Пирсона в диапазоне -1 до 0",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "В чем ключевое отличие Ridge от Lasso регрессии при прогнозировании оттока абонентов телеком-оператора с 50+ признаками?",
              "correct_answer": "Lasso обнуляет коэффициенты, Ridge уменьшает, не обнуляя",
              "var_1": "Lasso работает быстрее на больших данных телекома",
              "var_2": "Ridge обнуляет признаки, Lasso сохраняет все коэффициенты",
              "var_3": "Ridge использует L1-регуляризацию, Lasso использует L2-регуляризацию",
              "var_4": "Lasso обнуляет коэффициенты, Ridge уменьшает, не обнуляя",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Почему при построении модели скоринга для необанкинга следует использовать квантильную регрессию вместо OLS при наличии выбросов в данных о доходах?",
              "correct_answer": "Квантильная регрессия робастна к выбросам и моделирует условные квантили распределения",
              "var_1": "Квантильная регрессия минимизирует среднеквадратичную ошибку лучше чем OLS для финансовых данных",
              "var_2": "Квантильная регрессия робастна к выбросам и моделирует условные квантили распределения",
              "var_3": "Робастная регрессия методом Хубера полностью исключает выбросы из обучающей выборки",
              "var_4": "OLS с логарифмированием доходов нормализует распределение и устраняет влияние выбросов",
              "correct_position": 2
            }
          ]
        },
        {
          "theme": "Байесовская статистика и условные вероятности",
          "questions": [
            {
              "level": "Junior",
              "question": "Что показывает теорема Байеса при расчете вероятности мошеннической транзакции после получения сигнала от антифрод-системы?",
              "correct_answer": "Апостериорную вероятность мошенничества с учетом наблюдаемых данных",
              "var_1": "Апостериорную вероятность мошенничества с учетом наблюдаемых данных",
              "var_2": "Совместную вероятность мошенничества и срабатывания антифрод-системы",
              "var_3": "Функцию правдоподобия сигнала при условии мошеннической транзакции",
              "var_4": "Априорную вероятность мошенничества до анализа сигнала системы",
              "correct_position": 1
            },
            {
              "level": "Middle",
              "question": "В телеком-компании base rate оттока клиентов 2%, модель предсказывает отток с точностью 85%. Почему нельзя использовать только accuracy для оценки модели?",
              "correct_answer": "Несбалансированные классы приводят к высокой accuracy даже при неинформативных предсказаниях",
              "var_1": "Accuracy не учитывает стоимость ошибок первого и второго рода",
              "var_2": "Телеком-данные требуют использования взвешенной accuracy с учетом сегментов клиентов",
              "var_3": "Модель требует калибровки вероятностей для корректной работы с редкими событиями",
              "var_4": "Несбалансированные классы приводят к высокой accuracy даже при неинформативных предсказаниях",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "При построении байесовской модели скоринга в банке, как выбрать между информативными и неинформативными prior-распределениями для параметров кредитного риска в условиях ограниченных исторических данных?",
              "correct_answer": "Использовать слабо-информативные prior на основе экспертных оценок и регуляторных требований",
              "var_1": "Использовать слабо-информативные prior на основе экспертных оценок и регуляторных требований",
              "var_2": "Использовать conjugate prior на основе бета-распределения для упрощения вычислений",
              "var_3": "Задавать информативные prior из данных предыдущих моделей методом empirical Bayes",
              "var_4": "Применять строго неинформативные uniform prior для обеспечения объективности модели",
              "correct_position": 1
            }
          ]
        }
      ]
    },
    {
      "competency": "Python для Data Science (pandas, numpy, scikit-learn)",
      "type": "CORE",
      "importance": 90,
      "themes": [
        {
          "theme": "Манипуляция и обработка данных с pandas: индексация, группировка, объединение датафреймов",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой метод pandas используется для объединения двух датафреймов по общему столбцу, аналогично SQL JOIN?",
              "correct_answer": "merge() или pd.merge()",
              "var_1": "join() или df.join()",
              "var_2": "append() или pd.append()",
              "var_3": "merge() или pd.merge()",
              "var_4": "concat() или pd.concat()",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "В чем разница между merge() и concat() при объединении датафреймов с транзакциями клиентов банка из разных филиалов?",
              "correct_answer": "merge объединяет по ключам, concat склеивает по осям без условий",
              "var_1": "merge склеивает построчно, concat объединяет по общим столбцам",
              "var_2": "merge объединяет по ключам, concat склеивает по осям без условий",
              "var_3": "merge сохраняет дубликаты строк, concat удаляет повторяющиеся записи автоматически",
              "var_4": "concat требует общие ключи, merge работает с разными индексами",
              "correct_position": 2
            },
            {
              "level": "Senior",
              "question": "Почему при группировке 50 млн телеком CDR-записей по абонентам стоит использовать категориальные типы для phone_number перед groupby?",
              "correct_answer": "Снижает потребление памяти и ускоряет группировку за счет целочисленных кодов",
              "var_1": "Обеспечивает корректную сортировку телефонных номеров в международном формате при агрегации",
              "var_2": "Автоматически создает индексы для ускорения поиска конкретных абонентов в группах",
              "var_3": "Снижает потребление памяти и ускоряет группировку за счет целочисленных кодов",
              "var_4": "Активирует параллельную обработку groupby операций через многопоточность pandas",
              "correct_position": 3
            }
          ]
        },
        {
          "theme": "Векторизация и математические операции с numpy: Broadcasting, линейная алгебра, работа с массивами",
          "questions": [
            {
              "level": "Junior",
              "question": "Что произойдет при сложении numpy массива формы (5, 1) с массивом формы (3,) согласно правилам broadcasting?",
              "correct_answer": "Произойдет ошибка ValueError из-за несовместимости размерностей",
              "var_1": "Произойдет ошибка ValueError из-за несовместимости размерностей",
              "var_2": "Массив (3,) расширится до (5, 3) с дублированием",
              "var_3": "Получится массив формы (5, 3) с копированием элементов",
              "var_4": "Результат будет иметь форму (8,) после конкатенации",
              "correct_position": 1
            },
            {
              "level": "Middle",
              "question": "Для расчета корреляционной матрицы клиентских транзакций размером (10000, 50) что эффективнее: np.corrcoef или стандартизация с последующим np.dot, и почему?",
              "correct_answer": "np.dot после стандартизации быстрее за счет оптимизированного BLAS",
              "var_1": "np.cov с последующей нормализацией быстрее через vectorized операции",
              "var_2": "np.dot после стандартизации быстрее за счет оптимизированного BLAS",
              "var_3": "np.corrcoef эффективнее благодаря внутренней оптимизации для корреляций",
              "var_4": "pandas.DataFrame.corr() оптимальнее за счет Cython реализации",
              "correct_position": 2
            },
            {
              "level": "Senior",
              "question": "При векторизации расчета расстояний между 1 млн клиентов банка (shape 1000000, 128) память переполняется. Как оптимизировать без циклов, сохранив производительность numpy?",
              "correct_answer": "Использовать chunking с np.einsum или broadcasting блоками через np.add.outer",
              "var_1": "Применить np.memmap с файловым хранением и последующей агрегацией результатов",
              "var_2": "Переключиться на разреженные матрицы через scipy.sparse.csr_matrix для вычислений",
              "var_3": "Использовать chunking с np.einsum или broadcasting блоками через np.add.outer",
              "var_4": "Использовать np.float16 для массивов и scipy.spatial.distance.cdist с метрикой",
              "correct_position": 3
            }
          ]
        },
        {
          "theme": "Построение и оценка моделей машинного обучения с scikit-learn: Pipeline, кросс-валидация, метрики качества",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой класс scikit-learn используется для объединения препроцессинга и модели в единую последовательность трансформаций?",
              "correct_answer": "Pipeline",
              "var_1": "GridSearchCV",
              "var_2": "FeatureUnion",
              "var_3": "Pipeline",
              "var_4": "Transformer",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "Какую стратегию кросс-валидации следует использовать для временных рядов транзакций клиентов банка, чтобы избежать утечки данных из будущего?",
              "correct_answer": "TimeSeriesSplit, сохраняющий хронологический порядок данных",
              "var_1": "RepeatedKFold с несколькими повторениями разбиения",
              "var_2": "ShuffleSplit с перемешиванием для случайных выборок",
              "var_3": "StratifiedKFold для балансировки классов транзакций",
              "var_4": "TimeSeriesSplit, сохраняющий хронологический порядок данных",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Почему в Pipeline для модели скоринга клиентов телекома следует применять fit_transform на train и только transform на validation внутри кросс-валидации?",
              "correct_answer": "Предотвращает data leakage через статистики валидационного сета в обучающий",
              "var_1": "Ускоряет обучение через переиспользование fitted трансформеров на валидации",
              "var_2": "Обеспечивает консистентность гиперпараметров скейлеров между фолдами кросс-валидации",
              "var_3": "Сохраняет память через единственное обучение энкодеров на train",
              "var_4": "Предотвращает data leakage через статистики валидационного сета в обучающий",
              "correct_position": 4
            }
          ]
        },
        {
          "theme": "Предобработка данных и feature engineering: обработка пропусков, кодирование категориальных признаков, масштабирование",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой метод pandas используется для заполнения пропущенных значений в DataFrame константой?",
              "correct_answer": "fillna() с указанием значения для заполнения",
              "var_1": "dropna() с параметром fill_value вместо удаления",
              "var_2": "fillna() с указанием значения для заполнения",
              "var_3": "replace() с параметром missing_values для замены",
              "var_4": "interpolate() с методом constant для заполнения",
              "correct_position": 2
            },
            {
              "level": "Middle",
              "question": "В датасете банка есть категориальный признак 'регион' с 50 уникальными значениями и высокой кардинальностью. Какой метод кодирования предпочтительнее: OneHotEncoder или TargetEncoder, и почему?",
              "correct_answer": "TargetEncoder, так как избегает создания 50 признаков и curse of dimensionality",
              "var_1": "OrdinalEncoder, так как минимизирует размерность и совместим со всеми алгоритмами scikit-learn",
              "var_2": "LabelEncoder, так как сохраняет компактность данных и ускоряет обучение градиентного бустинга",
              "var_3": "OneHotEncoder, так как обеспечивает лучшую интерпретируемость модели для бизнес-аналитиков",
              "var_4": "TargetEncoder, так как избегает создания 50 признаков и curse of dimensionality",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "При построении пайплайна предобработки для скоринговой модели банка вы применяете SimpleImputer и StandardScaler. Почему критически важно fit этих трансформеров только на train выборке, а не на всем датасете?",
              "correct_answer": "Предотвращает data leakage - информация из test попадет в параметры трансформеров",
              "var_1": "Предотвращает data leakage - информация из test попадет в параметры трансформеров",
              "var_2": "Гарантирует идентичное распределение признаков между train и test выборками после масштабирования",
              "var_3": "Снижает вычислительную нагрузку при повторном применении трансформеров на новых данных",
              "var_4": "Обеспечивает корректную работу cross-validation при подборе гиперпараметров на train данных",
              "correct_position": 1
            }
          ]
        }
      ]
    },
    {
      "competency": "SQL и работа с базами данных",
      "type": "CORE",
      "importance": 85,
      "themes": [
        {
          "theme": "Основы SQL: SELECT-запросы, фильтрация, сортировка и агрегатные функции",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой оператор SQL используется для фильтрации строк по условию в запросе SELECT?",
              "correct_answer": "WHERE",
              "var_1": "HAVING",
              "var_2": "SELECT",
              "var_3": "WHERE",
              "var_4": "FILTER",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "В чем разница между WHERE и HAVING при работе с агрегатными функциями в запросах к таблице транзакций банка?",
              "correct_answer": "WHERE фильтрует строки до агрегации, HAVING фильтрует результаты после агрегации",
              "var_1": "WHERE фильтрует строки до агрегации, HAVING фильтрует результаты после агрегации",
              "var_2": "WHERE используется для индексированных полей, HAVING для вычисляемых столбцов таблицы",
              "var_3": "HAVING фильтрует строки перед группировкой, WHERE после выполнения агрегатных функций",
              "var_4": "WHERE применяется к GROUP BY, HAVING работает с JOIN и подзапросами",
              "correct_position": 1
            },
            {
              "level": "Senior",
              "question": "Почему запрос с COUNT(DISTINCT client_id) и GROUP BY на таблице с 500 млн транзакций телеком-оператора выполняется медленно, и как оптимизировать без изменения логики?",
              "correct_answer": "DISTINCT требует сортировки и дедупликации в памяти; решение через индексы, партиционирование или материализованные представления",
              "var_1": "Агрегатные функции с DISTINCT используют последовательное сканирование; решение через денормализацию таблицы и предрасчёт уникальных значений триггерами",
              "var_2": "GROUP BY создаёт временные таблицы на диске; решение через замену на оконные функции ROW_NUMBER с ORDER BY",
              "var_3": "COUNT(DISTINCT) блокирует параллельные запросы; решение через NOLOCK hints, увеличение памяти сервера и кэширование результатов",
              "var_4": "DISTINCT требует сортировки и дедупликации в памяти; решение через индексы, партиционирование или материализованные представления",
              "correct_position": 4
            }
          ]
        },
        {
          "theme": "JOIN-операции и подзапросы для объединения данных из множества таблиц",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой тип JOIN вернет все записи из левой таблицы клиентов банка и совпадающие записи из таблицы транзакций?",
              "correct_answer": "LEFT JOIN или LEFT OUTER JOIN",
              "var_1": "INNER JOIN для всех клиентов с транзакциями",
              "var_2": "LEFT JOIN или LEFT OUTER JOIN",
              "var_3": "RIGHT JOIN или RIGHT OUTER JOIN",
              "var_4": "FULL OUTER JOIN для объединения обеих таблиц",
              "correct_position": 2
            },
            {
              "level": "Middle",
              "question": "В чем разница между использованием подзапроса в WHERE и JOIN при объединении таблиц абонентов и их платежей в телеком-системе?",
              "correct_answer": "JOIN возвращает колонки обеих таблиц, подзапрос в WHERE только фильтрует строки основной таблицы",
              "var_1": "JOIN возвращает колонки обеих таблиц, подзапрос в WHERE только фильтрует строки основной таблицы",
              "var_2": "JOIN создает декартово произведение строк, подзапрос возвращает агрегированный результат для каждой строки",
              "var_3": "Подзапрос в WHERE поддерживает транзакции ACID, JOIN работает только в режиме READ UNCOMMITTED",
              "var_4": "Подзапрос в WHERE выполняется быстрее благодаря индексам, JOIN требует полное сканирование таблиц",
              "correct_position": 1
            },
            {
              "level": "Senior",
              "question": "Почему коррелированный подзапрос для расчета остатков по счетам клиентов может вызвать проблемы производительности и какую архитектуру выбрать вместо него?",
              "correct_answer": "Выполняется для каждой строки основного запроса. Использовать оконные функции или материализованные представления с инкрементальным обновлением",
              "var_1": "Создает временные таблицы в памяти. Использовать LEFT JOIN с GROUP BY и партицированием по дате транзакции",
              "var_2": "Нагружает кэш буферного пула. Денормализовать схему с триггерами для синхронизации остатков в отдельной таблице",
              "var_3": "Выполняется для каждой строки основного запроса. Использовать оконные функции или материализованные представления с инкрементальным обновлением",
              "var_4": "Блокирует таблицу на чтение. Применить INNER JOIN с агрегатными функциями и индексами на внешних ключах",
              "correct_position": 3
            }
          ]
        },
        {
          "theme": "Оконные функции (Window Functions) для аналитических вычислений",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой синтаксис используется для создания оконной функции ROW_NUMBER() для нумерации транзакций клиентов банка по дате?",
              "correct_answer": "ROW_NUMBER() OVER (PARTITION BY client_id ORDER BY transaction_date)",
              "var_1": "ROW_NUMBER() OVER (PARTITION BY client_id ORDER BY transaction_date)",
              "var_2": "ROW_NUMBER() OVER (ORDER BY transaction_date PARTITION BY client_id)",
              "var_3": "ROW_NUMBER(client_id, transaction_date) OVER (ORDER BY transaction_date)",
              "var_4": "ROW_NUMBER() GROUP BY client_id ORDER BY transaction_date WINDOW",
              "correct_position": 1
            },
            {
              "level": "Middle",
              "question": "В чем разница между ROWS BETWEEN и RANGE BETWEEN при вычислении скользящего среднего баланса абонентов телеком-оператора за последние 3 месяца?",
              "correct_answer": "ROWS учитывает физические строки, RANGE группирует одинаковые значения ORDER BY",
              "var_1": "ROWS применяет оконную функцию последовательно, RANGE использует параллельные вычисления для партиций",
              "var_2": "ROWS обрабатывает данные построчно, RANGE выполняет агрегацию всей партиции целиком",
              "var_3": "ROWS учитывает физические строки, RANGE группирует одинаковые значения ORDER BY",
              "var_4": "ROWS учитывает NULL значения в окне, RANGE исключает строки с пропусками",
              "correct_position": 3
            },
            {
              "level": "Senior",
              "question": "Почему при расчете процентилей доходов клиентов на миллионах записей следует использовать NTILE вместо комбинации ROW_NUMBER и подзапросов?",
              "correct_answer": "NTILE выполняется за один проход, не требует самосоединения и минимизирует сортировку",
              "var_1": "NTILE использует распределенные вычисления и параллельную обработку секций данных",
              "var_2": "NTILE кэширует промежуточные результаты сортировки для повторного использования в запросах",
              "var_3": "NTILE выполняется за один проход, не требует самосоединения и минимизирует сортировку",
              "var_4": "NTILE применяет оптимизацию индексов и автоматически создает материализованные представления",
              "correct_position": 3
            }
          ]
        },
        {
          "theme": "Оптимизация запросов, индексы и работа с планами выполнения",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой оператор SQL используется для просмотра плана выполнения запроса перед его фактическим исполнением?",
              "correct_answer": "EXPLAIN или EXPLAIN PLAN в зависимости от СУБД",
              "var_1": "DESCRIBE QUERY или QUERY PLAN в большинстве СУБД",
              "var_2": "SHOW PROFILE или SET STATISTICS для анализа производительности",
              "var_3": "EXPLAIN или EXPLAIN PLAN в зависимости от СУБД",
              "var_4": "ANALYZE или SHOW EXECUTION для просмотра статистики запроса",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "В таблице транзакций банка (50 млн записей) часто выполняется запрос по номеру карты и дате. Какой тип индекса оптимален и почему?",
              "correct_answer": "Составной индекс (card_number, date) для покрытия обоих условий фильтрации",
              "var_1": "Bitmap-индекс на card_number для компактного хранения и быстрого поиска",
              "var_2": "Кластерный индекс по дате для ускорения временных диапазонов",
              "var_3": "Составной индекс (card_number, date) для покрытия обоих условий фильтрации",
              "var_4": "Отдельные индексы на card_number и date для параллельной фильтрации",
              "correct_position": 3
            },
            {
              "level": "Senior",
              "question": "В DWH телеком-оператора запрос с JOIN трех больших таблиц (абоненты, тарифы, транзакции) выполняется 40 минут. План показывает Nested Loop. Как оптимизировать архитектурно и на уровне запроса?",
              "correct_answer": "Обновить статистику, добавить индексы на join-ключи, использовать Hash Join через hints, рассмотреть партиционирование и материализованные представления для агрегатов",
              "var_1": "Увеличить memory_limit и work_mem, добавить LIMIT в подзапросы, использовать Merge Join, денормализовать таблицы через дублирование данных в одну широкую таблицу",
              "var_2": "Обновить статистику, добавить индексы на join-ключи, использовать Hash Join через hints, рассмотреть партиционирование и материализованные представления для агрегатов",
              "var_3": "Добавить DISTINCT и ORDER BY в начало запроса, использовать временные таблицы для каждого JOIN, применить FORCE INDEX для всех таблиц",
              "var_4": "Перенести данные в NoSQL (MongoDB), использовать MapReduce для JOIN операций, создать копии таблиц с денормализацией для каждого типа запросов",
              "correct_position": 2
            }
          ]
        }
      ]
    },
    {
      "competency": "Построение классических моделей ML",
      "type": "CORE",
      "importance": 90,
      "themes": [
        {
          "theme": "Линейные модели и регуляризация (Linear Regression, Ridge, Lasso, ElasticNet)",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой метод sklearn используется для обучения линейной регрессии с L2-регуляризацией?",
              "correct_answer": "Ridge из sklearn.linear_model",
              "var_1": "Lasso из sklearn.linear_model",
              "var_2": "Ridge из sklearn.linear_model",
              "var_3": "SGDRegressor с alpha по умолчанию",
              "var_4": "LinearRegression с параметром penalty='l2'",
              "correct_position": 2
            },
            {
              "level": "Middle",
              "question": "В модели скоринга клиентов банка получили мультиколлинеарность между доходом и суммой депозита. Какую регуляризацию применить и почему?",
              "correct_answer": "Ridge, так как сохраняет все признаки, уменьшая их веса при мультиколлинеарности",
              "var_1": "Ridge, так как сохраняет все признаки, уменьшая их веса при мультиколлинеарности",
              "var_2": "Lasso, так как автоматически исключает коррелирующие признаки из модели",
              "var_3": "ElasticNet с альфа=1, чтобы полностью убрать один из коррелирующих признаков",
              "var_4": "PCA перед регрессией, чтобы преобразовать коррелирующие признаки в ортогональные компоненты",
              "correct_position": 1
            },
            {
              "level": "Senior",
              "question": "При построении модели прогноза оттока абонентов телеком-оператора с 500 признаками Lasso обнуляет 90% весов. Как оптимизировать pipeline для продакшена?",
              "correct_answer": "Использовать SelectFromModel для удаления нулевых признаков, затем переобучить Ridge на оставшихся",
              "var_1": "Использовать SelectFromModel для удаления нулевых признаков, затем переобучить Ridge на оставшихся",
              "var_2": "Увеличить альфа-параметр Lasso для получения более разреженной модели и ускорения инференса",
              "var_3": "Применить PCA для снижения размерности, затем использовать Lasso с теми же гиперпараметрами",
              "var_4": "Заменить Lasso на ElasticNet с l1_ratio=0.5 для баланса между разреженностью и производительностью",
              "correct_position": 1
            }
          ]
        },
        {
          "theme": "Ансамблевые методы и деревья решений (Random Forest, Gradient Boosting, XGBoost, LightGBM)",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой гиперпараметр в Random Forest контролирует количество деревьев в ансамбле?",
              "correct_answer": "n_estimators",
              "var_1": "max_features",
              "var_2": "max_depth",
              "var_3": "min_samples_split",
              "var_4": "n_estimators",
              "correct_position": 4
            },
            {
              "level": "Middle",
              "question": "Почему LightGBM быстрее XGBoost при обучении на датасете из 5 млн транзакций банка с 200 признаками?",
              "correct_answer": "Использует leaf-wise рост деревьев и histogram-based splitting для разбиения",
              "var_1": "Реализует распределённое обучение через Apache Spark для обработки транзакций",
              "var_2": "Применяет GPU-ускорение для параллельного вычисления градиентов на больших данных",
              "var_3": "Использует level-wise рост деревьев с оптимизированной работой в памяти",
              "var_4": "Использует leaf-wise рост деревьев и histogram-based splitting для разбиения",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Как оптимизировать Gradient Boosting для скоринговой модели с несбалансированными классами (1:50) при строгом требовании latency inference < 100ms?",
              "correct_answer": "Использовать scale_pos_weight, уменьшить max_depth до 3-4, применить early stopping и DART dropout",
              "var_1": "Увеличить n_estimators до 1000, применить GridSearchCV с cross-validation и настроить learning_rate",
              "var_2": "Использовать scale_pos_weight, уменьшить max_depth до 3-4, применить early stopping и DART dropout",
              "var_3": "Использовать class_weight='balanced', увеличить subsample до 1.0 и применить стекинг нескольких моделей",
              "var_4": "Применить SMOTE для балансировки, использовать deep trees с max_depth=10 и feature engineering",
              "correct_position": 2
            }
          ]
        },
        {
          "theme": "Метрики качества и валидация моделей (Cross-validation, ROC-AUC, Precision-Recall, RMSE)",
          "questions": [
            {
              "level": "Junior",
              "question": "Какая метрика показывает долю правильно предсказанных положительных классов среди всех предсказанных как положительные?",
              "correct_answer": "Precision (точность)",
              "var_1": "Recall (полнота)",
              "var_2": "Precision (точность)",
              "var_3": "Accuracy (правильность)",
              "var_4": "F1-score (гармоническое среднее)",
              "correct_position": 2
            },
            {
              "level": "Middle",
              "question": "Банк строит модель скоринга с сильным дисбалансом классов (дефолтов 2%). Какую метрику лучше использовать вместо accuracy и почему?",
              "correct_answer": "ROC-AUC или PR-AUC, так как accuracy будет завышена из-за дисбаланса классов",
              "var_1": "RMSE после применения SMOTE для балансировки классов перед обучением модели",
              "var_2": "ROC-AUC или PR-AUC, так как accuracy будет завышена из-за дисбаланса классов",
              "var_3": "Взвешенная accuracy с повышенным штрафом за ошибки на минорном классе дефолтов",
              "var_4": "F1-score с весами классов, так как она учитывает баланс между precision и recall",
              "correct_position": 2
            },
            {
              "level": "Senior",
              "question": "При валидации антифрод-модели телеком-оператора на временных данных обнаружили, что ROC-AUC на обучении 0.92, на валидации 0.88, но precision@top-5% упал с 0.75 до 0.45. Какие архитектурные проблемы это выявляет?",
              "correct_answer": "Переобучение на паттернах мошенничества, temporal data drift, или неадекватная калибровка вероятностей модели",
              "var_1": "Несбалансированность классов требует применения SMOTE или корректировки весов в функции потерь",
              "var_2": "Переобучение на паттернах мошенничества, temporal data drift, или неадекватная калибровка вероятностей модели",
              "var_3": "Высокая корреляция признаков и мультиколлинеарность снижают стабильность предсказаний на новых данных",
              "var_4": "Недостаточная глубина дерева решений и малое количество эстиматров в ансамбле моделей",
              "correct_position": 2
            }
          ]
        },
        {
          "theme": "Feature Engineering и предобработка данных (масштабирование, кодирование категорий, работа с пропусками)",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой метод sklearn используется для заполнения пропущенных значений средним или медианой в датасете клиентов банка?",
              "correct_answer": "SimpleImputer с параметрами strategy='mean' или strategy='median'",
              "var_1": "KNNImputer с параметром n_neighbors для интерполяции",
              "var_2": "fillna() метод pandas с параметром method='ffill'",
              "var_3": "SimpleImputer с параметрами strategy='mean' или strategy='median'",
              "var_4": "StandardScaler с параметром with_mean=True для заполнения",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "Вы строите модель скоринга для банка. Какой метод кодирования категорий выбрать для признака 'регион' с 17 областями Казахстана при высоком кардинале и риске переобучения?",
              "correct_answer": "Target Encoding или Frequency Encoding для снижения размерности",
              "var_1": "Binary Encoding для компактного представления 17 категорий",
              "var_2": "Target Encoding или Frequency Encoding для снижения размерности",
              "var_3": "Label Encoding с порядковым присвоением номеров областям",
              "var_4": "One-Hot Encoding с последующим PCA для уменьшения размерности",
              "correct_position": 2
            },
            {
              "level": "Senior",
              "question": "В телеком-компании признаки имеют разные распределения: трафик данных (степенное), длительность звонков (нормальное), баланс (с выбросами). Как спроектировать pipeline масштабирования для gradient boosting модели чтобы избежать data leakage?",
              "correct_answer": "RobustScaler внутри Pipeline с cross-validation, fit только на train fold",
              "var_1": "MinMaxScaler для каждого признака отдельно, fit на полных данных",
              "var_2": "StandardScaler на всём датасете перед split, затем обучение модели",
              "var_3": "RobustScaler внутри Pipeline с cross-validation, fit только на train fold",
              "var_4": "Gradient boosting не требует масштабирования, использовать сырые признаки",
              "correct_position": 3
            }
          ]
        }
      ]
    },
    {
      "competency": "MLOps инструменты (Airflow, MLflow)",
      "type": "DAILY",
      "importance": 70,
      "themes": [
        {
          "theme": "Организация и оркестрация ML-пайплайнов в Apache Airflow",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой оператор Airflow используется для запуска Python-функции в задаче ML-пайплайна?",
              "correct_answer": "PythonOperator",
              "var_1": "TaskFlowOperator",
              "var_2": "KubernetesPodOperator",
              "var_3": "PythonOperator",
              "var_4": "BashOperator",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "В чем разница между использования SubDagOperator и TaskGroup при организации сложного ML-пайплайна обработки транзакций?",
              "correct_answer": "TaskGroup группирует задачи визуально без создания отдельного DAG, SubDagOperator создаёт изолированный DAG",
              "var_1": "TaskGroup создаёт отдельный scheduler процесс, SubDagOperator использует общий executor",
              "var_2": "SubDagOperator обеспечивает параллельное выполнение задач, TaskGroup запускает последовательно",
              "var_3": "SubDagOperator группирует задачи логически, TaskGroup изолирует метаданные в отдельной базе",
              "var_4": "TaskGroup группирует задачи визуально без создания отдельного DAG, SubDagOperator создаёт изолированный DAG",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Почему динамическое создание задач через Dynamic Task Mapping предпочтительнее цикла при параллельном обучении моделей скоринга для разных регионов?",
              "correct_answer": "Задачи создаются в runtime, экономя память scheduler и позволяя масштабировать без перезапуска DAG",
              "var_1": "DAG с циклом требует ручной настройки pool slots для каждого региона перед запуском",
              "var_2": "Цикл блокирует executor threads, а Dynamic Mapping распределяет задачи через Celery workers автоматически",
              "var_3": "Задачи создаются в runtime, экономя память scheduler и позволяя масштабировать без перезапуска DAG",
              "var_4": "Dynamic Mapping использует XCom для передачи параметров моделей между задачами эффективнее циклов",
              "correct_position": 3
            }
          ]
        },
        {
          "theme": "Управление экспериментами и версионирование моделей в MLflow",
          "questions": [
            {
              "level": "Junior",
              "question": "Какая команда MLflow используется для логирования обученной модели машинного обучения в эксперимент?",
              "correct_answer": "mlflow.log_model() или mlflow.<framework>.log_model()",
              "var_1": "mlflow.save_model() или mlflow.export_model()",
              "var_2": "mlflow.register_model() с указанием пути",
              "var_3": "mlflow.track_model() или mlflow.log_artifact()",
              "var_4": "mlflow.log_model() или mlflow.<framework>.log_model()",
              "correct_position": 4
            },
            {
              "level": "Middle",
              "question": "В чем разница между версионированием моделей через MLflow Model Registry stages (Staging/Production) и версионированием через tags? Когда использовать каждый подход?",
              "correct_answer": "Stages управляют жизненным циклом и продвижением моделей, tags для метаданных и фильтрации",
              "var_1": "Stages для разграничения доступа команд, tags для управления версиями и роллбэков моделей",
              "var_2": "Stages обеспечивают Git-подобное ветвление моделей, tags фиксируют снимки для автоматических деплойментов",
              "var_3": "Stages управляют жизненным циклом и продвижением моделей, tags для метаданных и фильтрации",
              "var_4": "Tags определяют окружение развертывания, stages хранят метрики и параметры для сравнения экспериментов",
              "correct_position": 3
            },
            {
              "level": "Senior",
              "question": "При миграции скоринговой модели из MLflow в production у вас возникла проблема: модель в Registry показывает разные результаты при inference через mlflow.pyfunc.load_model() и через REST API. Какие три наиболее вероятные причины?",
              "correct_answer": "Различия в preprocessing pipeline, несовпадение версий зависимостей, разная сериализация входных данных",
              "var_1": "Различия в preprocessing pipeline, несовпадение версий зависимостей, разная сериализация входных данных",
              "var_2": "Разные эндпоинты для staging и production, отсутствие warm-up запросов, таймауты REST API",
              "var_3": "Различия в конфигурации MLflow Tracking Server, асинхронная загрузка модели, разные backend stores",
              "var_4": "Несоответствие flavors модели, кэширование артефактов на сервере, отличия в логировании метрик",
              "correct_position": 1
            }
          ]
        },
        {
          "theme": "Мониторинг и логирование ML-моделей в production",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой компонент MLflow используется для логирования метрик модели во время inference в production?",
              "correct_answer": "MLflow Tracking с методом log_metric",
              "var_1": "MLflow Projects с функцией monitor_model",
              "var_2": "MLflow Tracking с методом log_metric",
              "var_3": "MLflow Registry для tracking метрик",
              "var_4": "MLflow Models с методом log_inference",
              "correct_position": 2
            },
            {
              "level": "Middle",
              "question": "В чем разница между мониторингом data drift и concept drift при работе скоринговой модели в банке?",
              "correct_answer": "Data drift — изменение распределения входных данных, concept drift — изменение зависимости между признаками и целевой переменной",
              "var_1": "Data drift — изменение распределения входных данных, concept drift — изменение зависимости между признаками и целевой переменной",
              "var_2": "Data drift — расхождение между train и test выборками, concept drift — изменение распределения целевой переменной в продакшене",
              "var_3": "Data drift — отклонение предсказаний от исторических значений, concept drift — изменение структуры признаков в производственной базе",
              "var_4": "Data drift — изменение метрик качества модели, concept drift — изменение бизнес-требований к пороговым значениям скоринга",
              "correct_position": 1
            },
            {
              "level": "Senior",
              "question": "Модель оттока абонентов в телекоме показывает стабильные метрики качества, но бизнес-метрики упали. Какие проблемы мониторинга могли это пропустить?",
              "correct_answer": "Отсутствие мониторинга feature drift, задержек предсказаний, coverage модели и корреляции с бизнес-действиями",
              "var_1": "Неправильная настройка алертов в Grafana, отсутствие дашборда с confusion matrix и ROC-AUC",
              "var_2": "Отсутствие мониторинга feature drift, задержек предсказаний, coverage модели и корреляции с бизнес-действиями",
              "var_3": "Недостаточный мониторинг accuracy, precision, recall и F1-score на валидационной выборке",
              "var_4": "Отсутствие A/B тестирования, сравнения с baseline и регулярной переобучения модели на свежих данных",
              "correct_position": 2
            }
          ]
        },
        {
          "theme": "CI/CD для машинного обучения и автоматизация деплоя моделей",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой компонент MLflow используется для сохранения и версионирования обученных моделей машинного обучения?",
              "correct_answer": "MLflow Model Registry",
              "var_1": "MLflow Tracking Server",
              "var_2": "MLflow Artifact Store",
              "var_3": "MLflow Model Registry",
              "var_4": "MLflow Model Serializer",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "Как в Airflow настроить автоматический ретрейн модели скоринга при деградации метрик на 5% от baseline?",
              "correct_answer": "Использовать BranchPythonOperator с проверкой метрик и условным запуском DAG ретрейна",
              "var_1": "Использовать ExternalTaskSensor для мониторинга метрик и триггера отдельного pipeline",
              "var_2": "Создать Custom Operator с встроенной логикой сравнения и автоматическим schedule_interval",
              "var_3": "Использовать BranchPythonOperator с проверкой метрик и условным запуском DAG ретрейна",
              "var_4": "Настроить SLA в Airflow с email-алертами и ручным перезапуском DAG",
              "correct_position": 3
            },
            {
              "level": "Senior",
              "question": "Модель антифрода в production показывает latency 800ms вместо 200ms при A/B тесте. Какие узкие места проверить в пайплайне деплоя?",
              "correct_answer": "Сериализацию модели, отсутствие batch inference, избыточные feature transformations, network latency к feature store",
              "var_1": "Размер Docker image, compression алгоритмы для артефактов, logging verbosity level, monitoring overhead от Prometheus",
              "var_2": "Версию Python runtime, outdated TensorFlow dependencies, memory leaks в контейнере, CPU throttling limits",
              "var_3": "Сериализацию модели, отсутствие batch inference, избыточные feature transformations, network latency к feature store",
              "var_4": "Kubernetes pod autoscaling конфигурацию, replicas count, health check intervals, service mesh overhead в Istio",
              "correct_position": 3
            }
          ]
        }
      ]
    },
    {
      "competency": "Big Data (Hadoop/Spark)",
      "type": "DAILY",
      "importance": 65,
      "themes": [
        {
          "theme": "Архитектура Hadoop: HDFS, MapReduce и YARN",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой фактор репликации по умолчанию использует HDFS для хранения блоков данных?",
              "correct_answer": "Три реплики для каждого блока данных",
              "var_1": "Пять реплик для критичных данных",
              "var_2": "Одна реплика с возможностью масштабирования",
              "var_3": "Три реплики для каждого блока данных",
              "var_4": "Две реплики для обеспечения отказоустойчивости",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "В каких случаях следует использовать YARN вместо MapReduce для обработки телеметрии абонентов телеком-оператора?",
              "correct_answer": "Когда нужна интеграция Spark, Flink или других движков обработки",
              "var_1": "Когда MapReduce достигает лимита одновременных задач в очереди",
              "var_2": "Когда объем данных телеметрии превышает терабайт в сутки",
              "var_3": "Когда требуется реплицация данных между несколькими HDFS кластерами",
              "var_4": "Когда нужна интеграция Spark, Flink или других движков обработки",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Как диагностировать проблему медленного чтения транзакций клиентов банка из HDFS, если утилизация CPU и сети в норме?",
              "correct_answer": "Проверить IOPS дисков DataNode, балансировку блоков и долю локальных чтений",
              "var_1": "Настроить репликацию блоков на уровне rack awareness и compression codec",
              "var_2": "Проверить размер блоков HDFS и увеличить параллелизм MapReduce задач",
              "var_3": "Увеличить heap memory для NameNode и проверить GC паузы JVM",
              "var_4": "Проверить IOPS дисков DataNode, балансировку блоков и долю локальных чтений",
              "correct_position": 4
            }
          ]
        },
        {
          "theme": "Apache Spark: RDD, DataFrame и оптимизация производительности",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой метод используется для создания RDD из текстового файла в Spark?",
              "correct_answer": "sparkContext.textFile() или sc.textFile()",
              "var_1": "rdd.fromTextFile()",
              "var_2": "spark.read.text()",
              "var_3": "sparkContext.textFile() или sc.textFile()",
              "var_4": "sparkContext.readFile()",
              "correct_position": 3
            },
            {
              "level": "Middle",
              "question": "В каком случае следует использовать DataFrame вместо RDD при обработке транзакций банковских клиентов?",
              "correct_answer": "Когда данные структурированы и нужна оптимизация через Catalyst optimizer",
              "var_1": "Когда требуется низкоуровневый контроль над распределением данных по партициям",
              "var_2": "Когда нужна ленивая оценка выражений и трансформаций данных",
              "var_3": "Когда операции требуют сериализации через Java Serialization для производительности",
              "var_4": "Когда данные структурированы и нужна оптимизация через Catalyst optimizer",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Почему при обработке CDR-записей абонентов возникает skew на этапе join по customer_id и как это исправить?",
              "correct_answer": "Неравномерное распределение ключей. Использовать salting или broadcast join для малых таблиц",
              "var_1": "Неравномерное распределение ключей. Использовать salting или broadcast join для малых таблиц",
              "var_2": "Блокировка shuffle операций. Настроить spark.sql.shuffle.partitions и увеличить параллелизм executor'ов",
              "var_3": "Недостаточная память executor'ов. Увеличить spark.executor.memory и количество партиций",
              "var_4": "Проблема сериализации данных. Применить Kryo сериализацию и кеширование промежуточных результатов",
              "correct_position": 1
            }
          ]
        },
        {
          "theme": "Обработка потоковых данных с использованием Spark Streaming",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой метод используется для создания DStream из Kafka в Spark Streaming?",
              "correct_answer": "createDirectStream или createStream для интеграции с Kafka",
              "var_1": "fileStream с мониторингом директории Kafka логов",
              "var_2": "KafkaUtils.createRDD для пакетной обработки сообщений Kafka",
              "var_3": "socketTextStream с указанием Kafka брокера и топика",
              "var_4": "createDirectStream или createStream для интеграции с Kafka",
              "correct_position": 4
            },
            {
              "level": "Middle",
              "question": "В чем разница между window() и reduceByWindow() операциями при обработке транзакций банка в реальном времени?",
              "correct_answer": "window() возвращает все данные окна, reduceByWindow() агрегирует их функцией",
              "var_1": "window() группирует по ключам внутри окна, reduceByWindow() объединяет окна между партициями",
              "var_2": "window() создает временные срезы, reduceByWindow() выполняет инкрементальные вычисления с состоянием",
              "var_3": "window() агрегирует данные партициями, reduceByWindow() применяет функцию к микробатчам",
              "var_4": "window() возвращает все данные окна, reduceByWindow() агрегирует их функцией",
              "correct_position": 4
            },
            {
              "level": "Senior",
              "question": "Как решить проблему задержки обработки (processing delay > batch interval) при анализе CDR-записей телеком-оператора объемом 50K событий/сек?",
              "correct_answer": "Увеличить число партиций Kafka, параллелизм executors, уменьшить batch interval, оптимизировать shuffle",
              "var_1": "Мигрировать на Flink с event-time processing, включить checkpointing каждые 5 секунд для устойчивости",
              "var_2": "Увеличить batch interval до 30 секунд, добавить backpressure механизм, кэшировать справочники в памяти",
              "var_3": "Перейти на micro-batch архитектуру Storm с топологией подсчета метрик через Trident API",
              "var_4": "Увеличить число партиций Kafka, параллелизм executors, уменьшить batch interval, оптимизировать shuffle",
              "correct_position": 4
            }
          ]
        },
        {
          "theme": "Интеграция Big Data инструментов: Hive, HBase и экосистема Hadoop",
          "questions": [
            {
              "level": "Junior",
              "question": "Какой компонент Hadoop экосистемы позволяет выполнять SQL-подобные запросы к данным в HDFS?",
              "correct_answer": "Apache Hive",
              "var_1": "Apache Hive",
              "var_2": "HDFS Query Layer",
              "var_3": "Apache HBase",
              "var_4": "Apache Pig",
              "correct_position": 1
            },
            {
              "level": "Middle",
              "question": "Когда стоит использовать HBase вместо Hive для хранения транзакционных данных клиентов банка?",
              "correct_answer": "Когда требуется быстрый доступ по ключу и обновления в реальном времени",
              "var_1": "Когда требуется быстрый доступ по ключу и обновления в реальном времени",
              "var_2": "Когда необходимо хранить данные в колоночном формате для аналитики",
              "var_3": "Когда требуется пакетная обработка данных с использованием MapReduce заданий",
              "var_4": "Когда нужно выполнять SQL-запросы с агрегацией по историческим данным",
              "correct_position": 1
            },
            {
              "level": "Senior",
              "question": "Как решить проблему медленного выполнения JOIN между большой таблицей Hive с CDR данными и маленькой справочной таблицей HBase?",
              "correct_answer": "Использовать Map Join с кешированием справочника или Bulkload в Hive таблицу",
              "var_1": "Использовать Map Join с кешированием справочника или Bulkload в Hive таблицу",
              "var_2": "Использовать Secondary Index в HBase и распределенный JOIN через Reduce Side Join",
              "var_3": "Настроить репликацию HBase таблицы в HDFS и выполнить Shuffle Hash Join",
              "var_4": "Применить Bucket Map Join с партиционированием CDR таблицы по ключам HBase",
              "correct_position": 1
            }
          ]
        }
      ]
    }
  ]
}